# -*- coding: utf-8 -*-
"""YvonneZhang+Predicting Student Academic Success.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14AvMTK0ulDlresU8rl_2ELF68sxikOR0
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    roc_curve,
    classification_report,
    confusion_matrix
)

plt.rcParams["figure.figsize"] = (6, 4)
sns.set(style="whitegrid")

# Make sure the dataset file is in the same directory as this notebook
df = pd.read_csv("StudentsPerformance.csv")

df.info()
df[["math score", "reading score", "writing score"]].describe()

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
sns.histplot(df["math score"], bins=20, kde=True)
plt.title("Math Score Distribution")

plt.subplot(1, 3, 2)
sns.histplot(df["reading score"], bins=20, kde=True)
plt.title("Reading Score Distribution")

plt.subplot(1, 3, 3)
sns.histplot(df["writing score"], bins=20, kde=True)
plt.title("Writing Score Distribution")

plt.tight_layout()
plt.show()

sns.jointplot(
    x="reading score",
    y="writing score",
    data=df,
    kind="scatter",
    height=6
)
plt.suptitle("Reading vs Writing Score", y=1.02)
plt.show()

df["avg_score"] = (df["math score"] + df["reading score"] + df["writing score"]) / 3
df["pass"] = np.where(df["avg_score"] >= 60, 1, 0)


y = df["pass"]

X = df.drop(columns=[
    "math score",
    "reading score",
    "writing score",
    "avg_score",
    "pass"
])

X.head()

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape :", X_test.shape)

X_train_enc = pd.get_dummies(X_train, drop_first=True)
X_test_enc  = pd.get_dummies(X_test, drop_first=True)

X_test_enc = X_test_enc.reindex(columns=X_train_enc.columns, fill_value=0)

print("Encoded train shape:", X_train_enc.shape)
X_train_enc.head()

majority_class = y_train.mode()[0]
baseline_pred = np.full_like(y_test, fill_value=majority_class)

baseline_acc = accuracy_score(y_test, baseline_pred)
print("Baseline (majority class) accuracy:", baseline_acc)

log_model = LogisticRegression(max_iter=2000)
log_model.fit(X_train_enc, y_train)


y_pred_log = log_model.predict(X_test_enc)
y_prob_log = log_model.predict_proba(X_test_enc)[:, 1]

print("Logistic Regression accuracy:", accuracy_score(y_test, y_pred_log))
print("\nLogistic Regression classification report:")
print(classification_report(y_test, y_pred_log))

cm_log = confusion_matrix(y_test, y_pred_log)

plt.figure(figsize=(4,4))
sns.heatmap(cm_log, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix — Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

fpr_log, tpr_log, _ = roc_curve(y_test, y_prob_log)
auc_log = roc_auc_score(y_test, y_prob_log)

plt.figure(figsize=(4,4))
plt.plot(fpr_log, tpr_log, label=f"Logistic Regression (AUC = {auc_log:.2f})")
plt.plot([0, 1], [0, 1], "--")
plt.title("ROC Curve — Logistic Regression")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

rf_model = RandomForestClassifier(
    n_estimators=200,
    random_state=42
)
rf_model.fit(X_train_enc, y_train)

y_pred_rf = rf_model.predict(X_test_enc)
y_prob_rf = rf_model.predict_proba(X_test_enc)[:, 1]

print("Random Forest (basic) accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nRandom Forest (basic) classification report:")
print(classification_report(y_test, y_pred_rf))

cm_rf = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(4,4))
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Greens")
plt.title("Confusion Matrix — Random Forest (basic)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)
auc_rf = roc_auc_score(y_test, y_prob_rf)

plt.figure(figsize=(4,4))
plt.plot(fpr_rf, tpr_rf, label=f"Random Forest (AUC = {auc_rf:.2f})")
plt.plot([0, 1], [0, 1], "--")
plt.title("ROC Curve — Random Forest (basic)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

param_grid = {
    "n_estimators": [100, 200, 300],
    "max_depth": [None, 5, 10],
    "min_samples_split": [2, 5],
    "min_samples_leaf": [1, 2]
}

rf = RandomForestClassifier(random_state=42)

grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,
    scoring="roc_auc",
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_enc, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best CV AUC:", grid_search.best_score_)

best_rf_model = grid_search.best_estimator_

y_pred_opt = best_rf_model.predict(X_test_enc)
y_prob_opt = best_rf_model.predict_proba(X_test_enc)[:, 1]

print("Optimized Random Forest accuracy:", accuracy_score(y_test, y_pred_opt))
print("\nOptimized Random Forest classification report:")
print(classification_report(y_test, y_pred_opt))

cm_opt = confusion_matrix(y_test, y_pred_opt)

plt.figure(figsize=(4,4))
sns.heatmap(cm_opt, annot=True, fmt="d", cmap="Greens")
plt.title("Confusion Matrix — Random Forest (optimized)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

fpr_opt, tpr_opt, _ = roc_curve(y_test, y_prob_opt)
auc_opt = roc_auc_score(y_test, y_prob_opt)

plt.figure(figsize=(5,5))
plt.plot(fpr_log, tpr_log, label=f"Logistic Regression (AUC = {auc_log:.2f})")
plt.plot(fpr_opt, tpr_opt, label=f"Random Forest (optimized, AUC = {auc_opt:.2f})")
plt.plot([0, 1], [0, 1], "--")
plt.title("ROC Curve — Model Comparison")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.tight_layout()
plt.show()

importances = pd.DataFrame({
    "feature": X_train_enc.columns,
    "importance": best_rf_model.feature_importances_
}).sort_values("importance", ascending=False)

plt.figure(figsize=(6,6))
sns.barplot(
    x="importance",
    y="feature",
    data=importances.head(10),
    palette="Greens_r"
)
plt.title("Top 10 Most Important Features — Random Forest")
plt.tight_layout()
plt.show()

importances.head(10)

test_results = X_test.copy()
test_results["Actual_Pass"] = y_test.values
test_results["Predicted_Pass"] = y_pred_opt

errors = test_results[test_results["Actual_Pass"] != test_results["Predicted_Pass"]]
errors.head()

errors["parental level of education"].value_counts(normalize=True)